<!DOCTYPE html>

<html>
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<title>CINAHPA EBOOK</title>
		<link rel="stylesheet" href="assets/css/normalize.css">
		<link rel="stylesheet" href="assets/css/style.css">
		<link rel="stylesheet" href="assets/css/responsivo.css">
	</head>

<body>
		
		<header class="header">
			<div id="governo">
				<div>
					<p class="instituicoes">
					<span>Governo Federal</span>
					<span class="ponto">•</span>
					<spn>República Federativa do Brasil</span>
					<span class="ponto">•</span>
					<span>Ministério da Educação</span>
					<span class="ponto">•</span>
					<span>Coordenação de Aperfeiçoamento de Pessoal de Nível Superior</span>
					<span class="ponto">•</span>
					<span>Universidade Federal de Goiás</span>
				</div>
			</div>
			<div class="container">
				<div>
					<a class="marca" href="http://www.cinahpa.org" target="_blank"><img src="assets/img/marcas/cinahpa.png" alt="marcaufg"></a>
					<a href="index.html">
						<h1>A inovação emergente: tecnologias e interfaces</h1>

					</a>
				</div>
			</div>
		</header>
		
		<main>
			<section class="areaconteudo container" >
				<article>
				<h1>Mapeamento de Recursos Digitais de Tecnologia Assistiva: Interação multimodal para pessoas com deficiência visual e cegas</h1>
				<p>Dominique Leite Adam<input type="checkbox" id="nota1"><label for="nota1">1</label><span class="nota-rodape">Mestre em Design, Universidade Federal do Paraná, Curitiba, Paraná, Brasil. domiadam@gmail.com</span></p>
				<p>Maria Lucia Leite Ribeiro Okimoto<input type="checkbox" id="nota2"><label for="nota2">2</label><span class="nota-rodape">Drª em Engenharia, Universidade Federal do Paraná, Curitiba, Paraná, Brasil. lucia.demec@ufpr.br<input type="checkbox" id="nota3"><label for="nota3">3</label><span class="nota-rodape">Graduanda em Engenharia Mecânica, Universidade Federal do Paraná, Curitiba, Paraná, Brasil. izarpellon26@gmail.com</span></p>

				<p><strong>Resumo:</strong> Com base nos princípios do Design Inclusivo, este artigo visa mapear recursos digitais de Tecnologia Assistiva (TA) que auxiliam a interação de pessoas com deficiência visual ou cegas com o computador, associando-os à abordagem multimodal. Para isso, foi realizada uma Revisão Bibliográfica Sistemática no Portal de Periódicos da CAPES, visando identificar pesquisas dos últimos 5 anos. A busca constatou estudos que citam, desenvolvem ou avaliam recursos digitais assistivos que associam outputs visuais, sonoros e hápticos. Como resultado, verifica-se o potencial da abordagem multimodal no desenvolvimento destes recursos, possibilitando uma interação inclusiva.</p>

				<p><strong>Palavras-chave:</strong> recursos assistivos; multimodalidade; interação-humano computador; deficiência visual.</p>

				<h2>Introdução</h2>
				<p>A inclusão de pessoas com deficiência visual e cegas é intermediada por recursos de Tecnologia Assistiva (TA). Em muitos casos, este termo é associado à ideia de produtos inovadores que envolvem a mais alta tecnologia. No entanto, de acordo com o Comitê de Ajudas Técnicas (BRASIL, 2009), sua definição engloba diversos conceitos que tangenciam a acessibilidade, não sendo unicamente pautada no avanço tecnológico:</p>

				<blockquote>
					<p>Produtos, recursos, metodologias, estratégias, práticas e serviços que objetivam promover a funcionalidade, relacionada à atividade e participação, de pessoas com deficiência, incapacidades ou mobilidade reduzida, visando sua autonomia, independência, qualidade de vida e inclusão social (BRASIL, 2009, p. 9).</p>
				</blockquote>

				<p>Esta definição insere os recursos de TA como elementos fundamentais para a promoção da acessibilidade nos mais variados ambientes, sejam físicos ou digitais. Tornar acessível um recurso digital – acessibilidade digital – é utilizar a TA para promover a flexibilidade do artefato, permitindo que ele se adeque às necessidades do usuário. Segundo Conforto e Santarosa (2002) a TA auxilia a interação em diferentes níveis do mundo digital, desde <em>hardwares</em>, ao garantir o acesso ao computador e periféricos, a <em>softwares</em> e interfaces, ao possibilitar o diálogo com sistemas, navegadores e websites, por exemplo. Passerino e Montardo (2007) complementam que, para alcançar a acessibilidade digital, são necessárias ferramentas e adaptações embutidas tanto nos <em>hardwares</em> quanto nos <em>softwares</em>. Possibilitar meios distintos de interação capazes de fornecer o acesso ao conteúdo (físico ou digital) é um direito adquirido por todos (BRASIL, 2015).</p>
				<p>À vista disso, o presente artigo visa, por meio de um estudo exploratório, identificar recursos assistivos digitais que possibilitam a Interação Humano-Computador (IHC) acessível e discorrer a cerca da abordagem multimodal listando as possibilidades de interação provenientes de <em>outputs</em> sonoros e hápticos.</p>

				<h2>IHC e Recursos de TA</h2>
				<p>A Interação Humano-Computador (IHC) é uma área do conhecimento que visa desenvolver e avaliar soluções tecnológicas (sob a ótica da usabilidade e acessibilidade) considerando todos os aspectos de interação dos usuários com as interfaces/sistemas (CARROL e KJELDSKOV, 2013). Neste cenário, as interfaces digitais são recursos que intermediam a interação, permitindo que o usuário e o computador, mesmo utilizando linguagens completamente diferentes, se comuniquem. </p>
				<p>Para as pessoas com deficiência visual ou cegas, há uma série de recursos físicos, virtuais e digitais de TA que melhoram a qualidade de vida por possibilitarem a IHC por meio da estimulação sonora, háptica<input type="checkbox" id="nota3"><label for="nota3">3</label><span class="nota-rodape">O termo se refere na identificação de um ambiente ou objeto, físico ou virtual, por meio do tato e pela associação de aspectos cinestésicos (movimento e equilíbrio). Dessa forma é possível reconhecer a localização e posição no espaço, textura, superfície, temperatura, força e vibração (PARK e ALDERMAN, 2018) p. 261).</span> e cinestésica. Dentre os <em>hardwares</em> assistivos evidenciam-se o computador, tablets, dispositivos móveis, <em>wearables</em> e periféricos hápticos. Entre os softwares, destacam-se os leitores de tela, sintetizadores de voz e recursos hápticos (PARK e ALDERMAN, 2018). </p>
				<p>De acordo com Power <em>et al</em>. (2009) e Argyropoulos et al. (2019) a utilização de outputs sonoros como recursos assistivos pode ocorrer de duas formas: por meio da tecnologia dos leitores de tela, que traduzem a informação visual através de outputs sonoros sintetizados (<em>text-to-speech</em>), ou por meio de sistemas com o formato <em>Digital Accessible Information System</em> ­– DAISY, que fornecem a informação através de uma navegação sonora hierarquizada e sincronizada (<em>speech recording</em>). </p>
				<p>﻿Os recursos hápticos (tátil-vibracionais), segundo Caetano (2018) envolvem dispositivos capazes de oferecer <em>output</em> de força, textura, movimento e pressão. A utilização desses recursos visa criar sistemas que aproximam as sensações virtuais das físicas. Existem softwares que traduzem, por meio do código braille e suas variantes, um vasto grupo de materiais digitais. Textos, gráficos, imagens, símbolos, partituras musicais, notações matemáticas, entre outros podem ser acessados pela exploração háptica (POWER <em>et al</em>., 2009).</p> 

				<h2>Design Inclusivo e Multimodal</h2>
				<p>O avanço tecnológico em torno dos recursos de TA tem contribuído para a inclusão de pessoas com deficiência visual ou cegas na sociedade. Sob a ótica do Design Inclusivo, é possível desenvolver e adaptar interfaces com o intuito de torná-las acessíveis para um maior número de indivíduos, independente de suas limitações cognitivas. Para isso, de acordo com estudos de Bataliotti (2017) é necessário seguir a abordagem de Design Centrado no Usuário (DCU) junto às diretrizes e recomendações de acessibilidade e usabilidade (W3C, 2008) para que os recursos digitais cumpram seus objetivos, garantindo a inclusão. </p>
				<p>Ao projetar recursos de TA, é viável explorar as habilidades sensoriais dos potenciais usuários destes artefatos. A multimodalidade é um mecanismo amplamente utilizado para fomentar esta interação. De acordo com Park e Alderman (2018), ao explorar as modalidades visual, auditiva, háptica e cinestésica é possível elevar a capacidade de criação de mapas mentais dos ambientes em que estamos interagindo, sejam eles físicos ou virtuais, proporcionando interações e experiências mais significativas. </p>
				<p>A abordagem multimodal auxilia pessoas com deficiências, ao passo que a compensação da informação visual ocorre, principalmente, por meio da estimulação sonora e tátil. Ao associar outputs, a apresentação do conteúdo visual é substituída, complementada ou reforçada com base na necessidade do usuário (JAIMES; SEBE, 2007). </p>
				<p>Conveniente ressaltar que para desenvolver um recurso multimodal é necessário compreender que a interação entre os sentidos humanos e os <em>outputs</em> dos recursos estão emparelhados e, de modo cíclico, fornecem <em>feedback</em> a todo momento. Assim, para que a IHC seja eficiente, ressalta-se a importância de mapear o fluxo de tarefas que podem ser realizadas, o tempo de aprendizagem, bem como a demanda e sobrecarga cognitiva (PARK e ALDERMAN, 2018). </p>

				<h2>Procedimentos Metodológicos</h2>
				<p>Para nortear o desenvolvimento deste artigo, foram definidas 4 etapas principais, conforme Figura 1. A etapa inicial visou a familiarização com o tema, identificando as bases teóricas para fundamentar a discussão. A etapa 2 consistiu em uma Revisão Bibliográfica Sistemática da Literatura (RBS) com base em Conforto et al. (2011). O objetivo da revisão foi mapear pesquisas científicas que citam e utilizam recursos digitais de TA para mediar a interação da pessoa com deficiência visual e a interface. A etapa 3 catalogou os recursos assistivos identificados a fim compreendê-los sob a ótica da abordagem multimodal. Por fim, a etapa 4 objetivou discorrer a cerca da abordagem multimodal em recursos digitais de TA, listando a relevância da multimodalidade para a inclusão digital. </p>

				<figure class="galeria"><img src="assets/img/cap4/fig1.jpg" alt=""><figcaption>Figura 1. Estrutura metodológica. Fonte: os autores</figcaption></figure>

				<p>A partir da literatura sobre recursos digitais de TA e interação (etapa 1), foram selecionados termos chave relacionados ao <em>feedback</em> auditivo e háptico, tecnologia, interfaces e pessoa com deficiência visual ou cega para dar início à RBS. Para esta etapa foram estabelecidos critérios de busca a partir da seleção da Base de dados – Periódicos da CAPES (CAPES, 2019). Ressalta-se que esta revisão visa identificar artigos revisados por pares e publicados nos últimos cinco anos, garantindo conteúdo pertinente e atualizado sobre o tema. Para isso, foi utilizado como filtro de busca, a “busca avançada”, que permite a inserção dos termos chave em campos diferenciados, como título, autor, assunto ou qualquer campo. Desta forma, os documentos foram filtrados a partir da relevância dos termos e suas associações, utilizando operadores booleanos. O operador <em>AND</em> fornece resultados contendo, necessariamente, um termo e o outro; <em>OR</em> gera resultados contendo um termo ou o outro; e o asterisco (*) permite que variações de uma determinada palavra (a partir de seus radicais) estejam contidas nos resultados. Ressalta-se que a busca foi realizada em dois momentos: A busca (a) objetivou selecionar recursos assistivos digitais com <em>feedback</em> sonoro, e a busca (b), visou identificar recursos assistivos digitais com <em>feedback</em> háptico. Além desses fatores, para atingir um resultado direcionado, foi determinado que os termos relacionados às características do público alvo deveriam estar contidos nos títulos dos documentos. O Quadro 1 apresenta as <em>strings</em> de busca utilizadas, bem como os resultados encontrados a partir dos critérios estabelecidos.</p>

				<table>
					<caption>Quadro 1. <em>Strings</em> de busca avançada. Fonte: os autores.</caption>
				  <tr>
				    <th colspan="2">STRINGS DE BUSCA</th>
				    <th>RESULTADOS</th>
				  </tr>
				  <tr>
				    <td>(a)</td>
				    <td>Busca no campo título:<em> blind* OR (visually impaired)</em><br>
				    Busca em qualquer campo: <em>(audio OR (screen read*)) AND (software OR app OR application) AND development AND digital AND inclusion AND technolog*</em></td>
				    <td>52</td>
				  </tr>
				  <tr>
				    <td>(b)</td>
				    <td>Busca no campo título: <em>blind* OR (visually impaired)</em><br>
				    Busca em qualquer campo: <em>(haptic OR tactil*) AND (software OR app OR application) AND development AND digital AND inclusion AND technolog*</em></td>
				    <td>31</td>
				  </tr>
				  <tr>
				    <td colspan="2">TOTAL</td>
				    <td>83</td>
				  </tr>
				</table>

				<h2>Análise dos Resultados</h2>
				<p>Os 83 documentos encontrados foram filtrados a partir da leitura dos títulos, palavras-chave e resumo, sendo eleitos aqueles que se enquadram ao tema da RBS. Deste modo, na busca com a <em>string</em> (a), dos 52 artigos encontrados, 26 foram selecionados por trazerem a abordagem de TA e interfaces. Destes, 13 estudos abordam <em>softwares</em> e aplicativos para mediar a IHC inclusiva, seja no aporte teórico ou no desenvolvimento. Da mesma maneira, a busca com a <em>string</em> (b) resultou em 31 documentos. Dentre eles, 15 foram escolhidos por corresponderem ao tema investigado, 4 deles já mapeados com a <em>string</em> (a), restando 11 documentos para a análise. </p>
				<p>À vista disso, foram identificadas 24 pesquisas e 52 recursos digitais de TA que medeiam a IHC de pessoas com deficiência visual ou cegas. Os quadros 2, 3, 4, 5, 6, 7 e 8 apresentam a catalogação desses recursos por <em>output</em> sensorial, bem como os estudos em que são citados.</p>

				<table>
					<caption>Quadro 2. Recursos sonoros de TA – resultado da RBS. Fonte: os autores.</caption>
				  <tr>
					<th></th>
				    <th>RECURSO DE TA</th>
				    <th>DESCRIÇÃO</th>
				    <th>CITADO EM</th>
				  </tr>
				  <tr>
				    <td rowspan="23">SONORO</td>
				    <td><em>TalkBack</em></td>
				    <td><em>Software</em> leitor de tela</td>
				    <td>O’Sullivan; <em>et al.</em> (2013); Damaceno <em>et al.</em> (2016); Costa e Duarte (2017)</td>
				  </tr>
				  <tr>
				    <td><em>JAWS</em></td>
				    <td><em>Software</em> leitor de tela</td>
				    <td>Loiacono <em>et al.</em> (2012); Bidarra; Oyamada (2013); Fartaria <em>et al.</em> (2013); Baker; Green (2016); Costa e Duarte (2017); Maćkowski <em>et al.</em> (2018)</td>
				  </tr>
				  <tr>
				    <td><em>NonVisual Desktop (NVDA)</em></td>
				    <td><em>Software</em> leitor de tela</td>
				    <td>Fartaria <em>et al.</em> (2013); Baker; Green (2016); Maćkowski <em>et al.</em> (2018)</td>
				  </tr>
				  <tr>
				    <td><em>Leitor de tela programado em Python</em></td>
				    <td><em>Software</em> leitor de tela</td>
				    <td>Bidarra; Oyamada (2013)</td>
				  </tr>
				  <tr>
				    <td><em>VoiceOver</em></td>
				    <td><em>Software</em> leitor de tela</td>
				    <td>Guerreiro (2015); Ghidini <em>et al.</em> (2016); Costa e Duarte (2017)</td>
				  </tr>
				  <tr>
				    <td><em>All Appointments</em></td>
				    <td>Aplicativo de calendário para pessoas com deficiência visual</td>
				    <td>Ghidini <em>et al.</em> (2016)</td>
				  </tr>
				  <tr>
				    <td><em>AudioBrowser</em></td>
				    <td><em>Software</em> que permite a IHC por meio de comandos e feedbacks sonoros</td>
				    <td>Hakobyan <em>et al.</em> (2013)</td>
				  </tr>
				  <tr>
				    <td><em>IVONA (substituído pelo Amazon Polly)</em></td>
				    <td>Técnica de sintetização da fala humana, que converte texto em <em>output</em> sonoro</td>
				    <td>Costa e Duarte (2017)</td>
				  </tr>
				  <tr>
				    <td><em>MathPlayer, MathReader</em></td>
				    <td><em>Software</em> matemático que lê fórmulas escritas em MathML e nos DAISY Books</td>
				    <td>Depountis <em>et al.</em> (2015); <em>Maćkowski et al.</em> (2018)</td>
				  </tr>
				  <tr>
				    <td><em>Speed Rule Engine</em></td>
				    <td><em>Software</em> que converte fórmulas escritas em MathML para textos que podem ser lidos por leitores de tela convencionais</td>
				    <td>Maćkowski <em>et al.</em> (2018)</td>
				  </tr>
				  <tr>
				    <td><em>Window-eyes</em></td>
				    <td><em>Software</em> leitor de tela</td>
				    <td>Maćkowski <em>et al.</em> (2018)</td>
				  </tr>
				  <tr>
				    <td><em>MathSpeak</em></td>
				    <td>Sistema usado por leitores de tela para ler expressões matemáticas, como aquelas escritas em MathML</td>
				    <td>Depountis <em>et al.</em> (2015)</td>
				  </tr>
				  <tr>
				    <td><em>ClickHear</em></td>
				    <td><em>Software</em> de audiodescrição para internet</td>
				    <td>Depountis <em>et al.</em> (2015)</td>
				  </tr>
				  <tr>
				    <td><em>mBRAILLE</em></td>
				    <td><em>Software</em> que transpõe o sistema de escrita braile para interfaces digitais</td>
				    <td>Nahar <em>et al.</em> (2015)</td>
				  </tr>
				  <tr>
				    <td><em>VIRTUAL VISION</em></td>
				    <td><em>Software</em> de assistência sonora que possibilita a interação com os recursos computacionais</td>
				    <td>Carlos; De Sá (2018)</td>
				  </tr>
				  <tr>
				    <td><em>BlindHelper</em></td>
				    <td><em>Software</em> de mobilidade e navegação autônoma em museus</td>
				    <td>Meliones; Sampson (2018)</td>
				  </tr>
				  <tr>
				    <td><em>GOBLIN</em></td>
				    <td><em>Software</em> sintetizador de voz</td>
				    <td>Hudec; Smutny (2017)</td>
				  </tr>
				  <tr>
				    <td><em>Music Reader</em></td>
				    <td><em>Software</em> leitor de partituras musicais</td>
				    <td>Baker; Green (2016)</td>
				  </tr>
				  <tr>
				    <td><em>Mobile Speak</em></td>
				    <td><em>Software</em> leitor de telas</td>
				    <td>Baker; Green (2016)</td>
				  </tr>
				  <tr>
				    <td><em>The vOICe project</em></td>
				    <td>Sistema que traduz imagens para sons</td>
				    <td>Lahav <em>et al.</em> (2018)</td>
				  </tr>
				  <tr>
				    <td><em>CARA</em></td>
				    <td>Sistema de assistência domiciliar, locomoção</td>
				    <td>Liu <em>et al.</em> (2018)</td>
				  </tr>
				  <tr>
				    <td><em>Microsoft HoloLens</em></td>
				    <td>Sistema de assistência a mobilidade baseado em realidade virtual</td>
				    <td>Liu <em>et al.</em> (2018)</td>
				  </tr>
				  <tr>
				    <td><em>IC2D</em></td>
				    <td>Sistema para desenho digital</td>
				    <td><em>Mukherjee et al.</em> (2014)</td>
				  </tr>
				</table>

				<table>
					<caption>Quadro 3. Recursos visuais de TA – resultado da RBS. Fonte: os autores.</caption>
					  <tr>
					  	<th></th>
					    <th>RECURSO DE TA</th>
					    <th>DESCRIÇÃO</th>
					    <th>CITADO EM</th>
					  </tr>
					  <tr>
					    <td>VISUAL</td>
					    <td>iZoom, MAGIC, SuperNova, ZoomText</td>
					    <td>Software de visualização e ampliação</td>
					    <td>Baker; Green (2016)</td>
					  </tr>
				</table>

				<table>
					<caption>Quadro 4. Recursos hápticos de TA – resultado da RBS. Fonte: os autores.</caption>
					  <tr>
					  	<th></th>
					    <th>RECURSO DE TA</th>
					    <th>DESCRIÇÃO</th>
					    <th>CITADO EM</th>
					  </tr>
					  <tr>
					    <td rowspan="3">HÁPTICO</td>
					    <td><em>Slide Rule</em></td>
					    <td>Sistema de interação baseada em gestos de deslizamentos na tela</td>
					    <td>Hakobyan <em>et al.</em> (2013); Damaceno <em>et al.</em> (2016)</td>
					  </tr>
					  <tr>
					    <td><em>UbiBraille</em></td>
					    <td>Conjunto de <em>software</em> e <em>hardware</em> que permite a leitura de caracteres em braille por meio de vibração</td>
					    <td>Guerreiro (2015)</td>
					  </tr>
					  <tr>
					    <td><em>HoliBraille</em></td>
					    <td>Conjunto de <em>software</em> e <em>hardware</em> que permite a interação com dispositivos móveis por meio de vibrações</td>
					    <td>Guerreiro (2015)</td>
					  </tr>
				</table>

				<table>
					<caption>Quadro 5. Recursos multimodais (visual e sonoro) de TA – resultado da RBS. Fonte: os autores.</caption>
					  <tr>
					    <th></th>
					    <th>RECURSO DE TA</th>
					    <th>DESCRIÇÃO</th>
					    <th>CITADO EM</th>
					  </tr>
					  <tr>
					    <td rowspan="4">VISUAL E SONORO</td>
					    <td><em>xLupa</em></td>
					    <td>Aplicativo para Linux que inclui ampliador e leitor de tela e calibragem de vídeo</td>
					    <td>Bidarra; Oyamada (2013)</td>
					  </tr>
					  <tr>
					    <td><em>NavMol 2.0</em></td>
					    <td><em>Software</em> educativo que visa o ensino de química a pessoas com deficiência visual</td>
					    <td>Fartaria et al. (2013); Lahav <em>et al.</em> (2018)</td>
					  </tr>
					  <tr>
					    <td><em>MAPVOICE</em></td>
					    <td><em>Software</em> para auxílio na aprendizagem da Cartografia</td>
					    <td>Carlos; De Sá (2018)</td>
					  </tr>
					  <tr>
					    <td><em>IVEO Viewer</em></td>
					    <td><em>Software</em> de visualização e ampliação de imagens</td>
					    <td>Mukherjee <em>et al.</em> (2014)</td>
					  </tr>
				</table>

				<table>
					<caption>Quadro 6. Recursos multimodais (sonoro e háptico) de TA – resultado da RBS. Fonte: os autores.</caption>
					<tr>
					    <th></th>
					    <th>RECURSO DE TA</th>
					    <th>DESCRIÇÃO</th>
					    <th>CITADO EM</th>
					  </tr>
					  <tr>
					    <td rowspan="11">SONORO E HÁPTICO</td>
					    <td><em>Foogue</em></td>
					    <td>Um sistema de IHC que utiliza recursos sonoros e hápticos</td>
					    <td>Hakobyan <em>et al.</em> (2013)</td>
					  </tr>
					  <tr>
					    <td><em>MoBraille</em></td>
					    <td><em>Hardware</em> que permite a leitura e a digitação de caracteres em smartphones</td>
					    <td>Hakobyan <em>et al.</em> (2013)</td>
					  </tr>
					  <tr>
					    <td><em>Lambda</em></td>
					    <td><em>Software</em> que converte fórmulas matemáticas em sinais a serem lidos por alguns dispositivos com <em>feedback</em> háptico</td>
					    <td>Maćkowski <em>et al.</em> (2018)</td>
					  </tr>
					  <tr>
					    <td><em>Braille note-taker</em></td>
					    <td><em>Hardware</em> para o registro de informações e posterior impressão em braille</td>
					    <td>Baker; Green (2016)</td>
					  </tr>
					  <tr>
					    <td><em>Talking Tactile Tablets</em></td>
					    <td>Hardware para visualizar gráficos e imagens</td>
					    <td>Mukherjee <em>et al.</em> (2014); Lahav et al. (2018)</td>
					  </tr>
					  <tr>
					    <td><em>Line Graphs Technology</em></td>
					    <td>Sistema para o aprendizado de matemática</td>
					    <td>Lahav <em>et al.</em> (2018)</td>
					  </tr>
					  <tr>
					    <td><em>TDraw</em></td>
					    <td><em>Software</em> que transpõe o desenho feito pela caneta digitalizadora para a tela</td>
					    <td>Mukherjee <em>et al.</em> (2014)</td>
					  </tr>
					  <tr>
					    <td><em>Touch Tiles</em></td>
					    <td><em>Software</em> para o ensino de geometria</td>
					    <td>Mukherjee <em>et al.</em> (2014)</td>
					  </tr>
					  <tr>
					    <td><em>Audio Tact</em></td>
					    <td><em>Software</em> matemático (funções e gráficos)</td>
					    <td>Mukherjee <em>et al.</em> (2014)</td>
					  </tr>
					  <tr>
					    <td><em>Sparsha System</em></td>
					    <td>Sistema de tradução da língua Indiana para o braille (impresso)</td>
					    <td>Mukherjee <em>et al.</em> (2014)</td>
					  </tr>
					  <tr>
					    <td><em>Qiktac</em></td>
					    <td><em>Software</em> para criação de gráficos para serem impressos (<em>emboss</em>)</td>
					    <td>Mukherjee <em>et al.</em> (2014)</td>
					  </tr>
				</table>

				<table>
					<caption>Quadro 7. Recursos multimodais (visual e háptico) de TA – resultado da RBS. Fonte: os autores.</caption>
					  <tr>
					    <th></th>
					    <th>RECURSO DE TA</th>
					    <th>DESCRIÇÃO</th>
					    <th>CITADO EM</th>
					  </tr>
					  <tr>
					    <td rowspan="2">VISUAL E HÁPTICO</td>
					    <td><em>Servidor tátil programado em linguagem C</em></td>
					    <td>Servidor táctil programado em C que se comunica com um pequeno motor no mouse e o faz vibrar</td>
					    <td>Bidarra; Oyamada (2013)</td>
					  </tr>
					  <tr>
					    <td><em>Wiimote</em></td>
					    <td>Controle do videogame Wii, da Nintendo</td>
					    <td>Costa e Duarte (2017)</td>
					  </tr>
				</table>

				<table>
					<caption>Quadro 8. Recursos multimodais (visual, sonoro e háptico) de TA – resultado da RBS. Fonte: os autores.</caption>
					  <tr>
					    <th></th>
					    <th>RECURSO DE TA</th>
					    <th>DESCRIÇÃO</th>
					    <th>CITADO EM</th>
					  </tr>
					  <tr>
					    <td rowspan="3">SONORO, VISUAL E HÁPTICO</td>
					    <td><em>V-Braille</em></td>
					    <td><em>Software</em> que permite a leitura de caracteres (célula braile) por meio de vibrações e feedback sonoro (text-to-speech)</td>
					    <td>Hakobyan <em>et al. </em>(2013)</td>
					  </tr>
					  <tr>
					    <td><em>SensiTV</em></td>
					    <td>Plataforma que utiliza luz, vibrações e música de fundo para expressar emoções contidas em filmes</td>
					    <td>Costa e Duarte (2017)</td>
					  </tr>
					  <tr>
					    <td><em>IVEO</em></td>
					    <td>Conjunto de <em>software</em> e <em>hardware</em> que permite a leitura de desenhos e diagramas impressos em relevo</td>
					    <td>Mukherjee <em>et al.</em> (2014); Depountis <em>et al.</em> (2015); Maćkowski <em>et al.</em> (2018)</td>
					  </tr>
				</table>
				<p>Em tese, há uma enorme preocupação em tornar a IHC mais adequada ao usuário, seja por meio da utilização de <em>outputs</em> sonoros, hápticos ou visuais (para aqueles com baixa visão, por exemplo). Em geral, foi possível observar diferentes abordagens sobre os recursos digitais de TA que facilitam a interação, estas diretamente relacionadas aos objetivos das pesquisas mapeadas. Alguns estudos trazem apenas referências de <em>softwares/hardwares</em> assistivos (BAKER e GREEN, 2016; LAHAV <em>et al.</em>, 2018), outros abordam o desenvolvimento de soluções de TA (FARTARIA <em>et al.</em>, 2013; GHIDINI <em>et al.</em>, 2016; HUDEC e SMUTNY, 2017; LIU <em>et al.</em>, 2018), ou ainda a avaliação destas por meio de estudos experimentais (KUMAR e SANAMAN, 2015; GUERREIRO, 2015; DAMACENO <em>et al.</em>, 2016) a fim de comprovar sua eficácia e eficiência na mediação da IHC de pessoas com deficiência visual ou cegas. </p>
				<p>De modo geral, os recursos de TA detectados auxiliam em diversas atividades, seja na navegação com o meio digital e tarefas cotidianas <em>(All Appointments, Audio Browser, Sensi TV, DOSVOX , Click Hear)</em>, no desenvolvimento e interpretação de imagens e gráficos <em>(IVEO, vOICe Project, IC2D, TDraw, Touch Tiles, QikTac)</em>; no ensino-aprendizagem de matemática <em>(MathPlayer, MathReader, MathSpeak, Lambda, Speed Rule Engine)</em>, química <em>(NavMol 2.0)</em>, cartografia <em>(Map Voice)</em>, braille <em>(V-Braille, MoBraille, UbiBraille,HoliBraille, mBraille)</em>, música <em>(Music Reader)</em>, como também no deslocamento e navegação em ambientes físicos (<em>Blind Helper, CARA, Microsoft HoloLens)</em>. </p>
				<p>A Figura 2 ilustra os recursos a partir de seu <em>output</em> de interação. É possível observar que dos 52 recursos digitais de TA, a maioria (n=43) apresenta <em>output</em> sonoro, como os sintetizadores de voz (<em>e.g. NavMol 2.0, GOBLIN</em>) e leitores de tela (<em>e.g. JAWS, NVDA, Voice Over, Window-eyes, Molibe Speak</em>). Isso pode ser justificado pelo fato da audição ser a habilidade sensorial primária desenvolvida pelas pessoas com deficiência visual ou cegas (TAKAGI <em>et al</em>., 2007). </p>

				<figure class="galeria"><img src="assets/img/cap4/fig2.jpg" alt=""><figcaption>Figura 2. <em>Outputs</em> de interação dos recursos digitais de TA mapeados. Fonte: os autores.</figcaption></figure>

				<p>Na sequência, 13 recursos apresentam <em>output</em> visual, sendo que apenas 3 são demostrados em uma única modalidade (ampliadores de tela). Em relação ao <em>output</em> háptico, por meio de <em>feedback</em> de força ou vibração, foram identificados 19 recursos assistivos, como por exemplo o <em>Side Rule, Servidor Tátil programado em linguagem C e o Braile note-taker</em>. Assim como os recursos somente visuais, o <em>output</em> háptico é normalmente associado a outra modalidade, a fim de complementar e reforçar a interação – abordagem multimodal (JAIMES e SEBE, 2007).</p>
				<p>Nota-se que cerca de 38% dos recursos mapeados possuem a abordagem multimodal associando ao menos dois outputs sensoriais distintos. Estudos indicam que as interações multimodais – visual-sonora (<em>e.g xLupa, MAPVOICE, IVEO Viewer, NavMol 2.0); visual-háptica (e.g. Servidor tátil programado em linguagem C , Wiimote)</em> e visual-sonora-háptica <em>(e.g. SensiTV, IVEO, V-Braille)</em> tendem a favorecer a IHC ao passo que proporcionam o acesso inclusivo à interface (HAKOBYAN <em>et al.</em>, 2013; MUKHERJEE <em>et al.</em>, 2014; COSTA e DUARTE, 2017).</p>
				<p>Neste contexto ressalta-se a tríade multimodal. Embora identificada em apenas três recursos assistivos, a associação de <em>outputs</em> sonoros, visuais e hápticos apresenta grande potencial para mediar uma a interação inclusiva, independente do objetivo e do contexto em que ocorre. O <em>V-Braille</em>, por exemplo, é capaz de integrar os <em>outputs</em> sensoriais a fim de ensinar a linguagem braile de forma objetiva (compreensão de caracteres, leitura e escrita) e lúdica (por meio de jogos) para pessoas com ou sem deficiência visual. Já o <em>Sensi-TV</em> apresenta uma abordagem multimodal para o entretenimento, que possibilita a tradução de emoções contidas em filmes. A interação pode acontecer por meio da associação visual (<em>e.g.</em> imagens, legenda, variação de luminosidade), tátil (<em>e.g.</em> frequências de vibrações) e sonoro (e.g. padrões musicais), resultando em experiências ricas e inclusivas. Por fim, o recurso <em>IVEO</em> permite a transposição de conteúdos visuais (digitais ou não) em informações sonoras (leitor de tela) e táteis (interação háptica). Este recurso apresenta grande potencial para ser utilizado no aprendizado de conteúdos imagéticos (<em>e.g.</em> mapas, gráficos, ilustrações e esquemas visuais), pois possibilita a exploração visual e tátil de figuras, seguida de informações sonoras que descrevem a área percebida (<em>e.g.</em> legenda, descrição sonora). </p> 

				<h2>Discussão e Considerações</h2>
				<p>É possível constatar que a maioria dos recursos identificados fornecem o <em>output</em> sonoro, vindo de encontro com a habilidade sensorial melhor desenvolvida pelas pessoas com deficiência visual. Como complemento para a interação, nota-se que muitos recursos apresentam mais de um modo de fornecimento de dados, confirmando o potencial da multimodalidade para desenvolver e/ou adaptar interfaces e sistemas, tornando-os perceptíveis a um maior número de indivíduos.</p>
				<p>Simultaneamente, observa-se a cerca das limitações econômicas, tecnológicas, projetuais e de implementação desses recursos digitais de TA. Considerando a sociedade brasileira, nota-se que estes recursos são direcionados a um nicho específico de mercado que, muitas vezes, requerem um elevado custo de implementação e treinamento para o uso, levando-os à obsolência. Em contrapartida, estes fatores motivam pesquisadores a proporem as melhores soluções, personalizadas muitas vezes,  a partir das condições a eles fornecidas.</p>
				<p>A partir deste estudo conclui-se que a multimodalidade está em constante evolução e é amplamente utilizada em recursos assistivos. A partir dos estudos mapeados é possível inferir que ao aliar as necessidades dos usuários à tecnologia, comprova-se a oportunidade de explorar a associação sensorial para promover uma IHC inclusiva em recursos digitais de TA. Constatou-se que ao fornecer mais de um <em>output</em> sensorial, maior é a possibilidade de este recurso ser inclusivo. Contudo, reitera-se a necessidade de, ao desenvolver um recurso assistivo multimodal, considerar a abordagem do Design Centrado no Usuário a fim de adequar as funções aos objetivos de interação, com o intuito de evitar a sobrecarga cognitiva. À vista disso, amplia-se o diálogo do indivíduo com o mundo, permitindo que as informações textuais, imagéticas, audiovisuais ou cinestésicas, sejam apreeendidas através dos sentidos mais desenvolvidos. </p>
				<p>Desta forma, conclui-se que a multimodalidade tem o potencial de fornecer experiências de interação mais ricas e não exclusivas às pessoas com deficiência. A interação proporcionada influencia na qualidade de vida, independência e inclusão de todos à sociedade.</p>

				<h2>Referências</h2>
				<p>ARGYROPOULOS, V.; PAVELI, A.; NIKOLARAIZI, M. The role of DAISY digital talking books in the education of individuals with blindness: A pilot study. <strong>Education and Information Technologies</strong>, v. 24, n. 1, p. 693–709, 2019. Disponível em: <a href="https://doi.org/10.1007/s10639-018-9795-2" target="blank">https://doi.org/10.1007/s10639-018-9795-2</a>. Acesso em: 14/6/2019.</p>
				<p>BAKER, D.; GREEN, L. Perceptions of schooling, pedagogy and notation in the lives of visually-impaired musicians. <strong>Research Studies in Music Education</strong>, v. 38, n. 2, p. 193–219, 2016. Disponível em: <a href="http://vimusicians.ioe." target="blank">http://vimusicians.ioe.</a>. Acesso em: 14/6/2019.</p>
				<p>BATALIOTTI, S. E. <strong>Da acessibilidade à autonomia do usuário com deficiência visual em ambientes virtuais de aprendizagem</strong>, 2017. Universidade Estadual Paulista.</p>
				<p>BIDARRA, J.; OYAMADA, M. S. Development of an Interactive Kiosk with Screen Amplifier for the Elderly and Those with Low Vision. , v. 45, n. 2, p. 117–133, 2013.</p>
				<p>BRASIL, C. DE A. T. <strong>Tecnologia Assistiva.</strong> Brasília, 2009.</p>
				<p>BRASIL, P. DA R. <strong>Lei Brasileira de Inclusão da Pessoa com Deficiência.</strong> 2015.</p>
				<p>CAETANO, A. C. M. Interfaces hápticas dispositivos não convencionais de interação. Anais do 7o Encontro Internacional de Arte e Tecnologia. 2018. Brasilia.</p>
				<p>CAPES, C. DE A. DE P. DE N. S. Portal de Periodicos da CAPES. Disponível em: <a href="http://www.periodicos.capes.gov.br/" target="blank">http://www.periodicos.capes.gov.br/</a>. Acesso em: 17/6/2019.</p>
				<p>CARLOS, L. B.; DE SÁ, L. A. C. M. MAPVOICE: Ferramenta Computacional para Auxílio na aprendizagem da Cartografia para Deficientes Visuais. <strong>Bulletin of Geodetic Sciences</strong>, v. 24, p. 58–68, 2018. Disponível em: <a href="http://english.tse.jus.br/arquivos/federal-constitution/view." target="blank">http://english.tse.jus.br/arquivos/federal-constitution/view.</a>. Acesso em: 14/6/2019.</p>
				<p>CARROL, J. M. .; KJELDSKOV, J. 2. Human Computer Interaction. <strong>The Encyclopedia of Human-Computer Interaction</strong>, 2013. Addison-Wesley Pub. Co. Disponível em: <a href="https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro" target="blank">https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro</a>. Acesso em: 16/6/2019.</p>
				<p>CONFORTO, D.; SANTAROSA, L. M. C. Acessibilidade à web : internet para todos. <strong>Revista de Informática na Educaçao: Teoria, Prática</strong>, v. 5, p. 87–102, 2002. Disponível em: <a href="http://edu3051.pbworks.com/f/ACESSIBILIDADE_WEB_revista_PGIE.pdf" target="blank">http://edu3051.pbworks.com/f/ACESSIBILIDADE_WEB_revista_PGIE.pdf</a>.</p>
				<p>CONFORTO, E. C.; AMARAL, D. C.; LUIS DA SILVA, S. Roteiro para revisão bibliográfica sistemática: aplicação no desenvolvimento de produtos e gerenciamento de projetos. 8o Congresso Brasileiro de Gestão de Desenvolvimento de Produto CBGDP. <strong>Anais...</strong>, 2011. Porto Alegre. Disponível em: <a href="http://vision.ime.usp.br/~acmt/conforto.pdf" target="blank">http://vision.ime.usp.br/~acmt/conforto.pdf</a>. Acesso em: 12/6/2019.</p>
				<p>COSTA, D.; DUARTE, C. Visually impaired people and the emerging connected TV: a comparative study of TV and Web applications’ accessibility. <strong>Universal Access in the Information Society</strong>, v. 16, n. 1, p. 197–214, 2017. Disponível em: <a href="http://link.springer.com/10.1007/s10209-016-0451-6" target="blank">http://link.springer.com/10.1007/s10209-016-0451-6</a>. Acesso em: 13/9/2018.</p>
				<p>DAMACENO, R. J. P.; BRAGA, J. C.; CHALCO, J. P. M. Mobile Device Accessibility for the Visually Impaired. <strong>Proceedings of the 15th Brazilian Symposium on Human Factors in Computer Systems - IHC ’16</strong>, October 2017, p. 1–10, 2016. Disponível em: <a href="http://dl.acm.org/citation.cfm?doid=3033701.3033703" target="blank">http://dl.acm.org/citation.cfm?doid=3033701.3033703</a>.</p>
				<p>DEPOUNTIS, V. M.; POGRUND, R. L.; GRIFFIN-SHIRLEY, N.; LAN, W. Y. Technologies Used in the Study of Advanced Mathematics by Students Who Are Visually Impaired in Classrooms: Teachers’ Perspectives. <strong>Journal of Visual Impairment & Blindness</strong>, 2018, v.109, p-﻿265-278 .</p>
				<p>FARTARIA, R. P. S.; PEREIRA, F.; BONIFÁCIO, V. D. B.; et al. <em>NavMol 2.0</em> - A Molecular Structure Navigator/Editor for Blind and Visually Impaired Users. <strong>European Journal of Organic Chemistry</strong>, v. 2013, n. 8, p. 1415–1419, 2013. Wiley-Blackwell. Disponível em: <a href="http://doi.wiley.com/10.1002/ejoc.201201458" target="blank">http://doi.wiley.com/10.1002/ejoc.201201458</a>. Acesso em: 27/9/2018.</p>
				<p>GHIDINI, E.; ALMEIDA, W. D. L.; MANSSOUR, I. H.; SILVEIRA, M. S. Developing Apps for Visually Impaired People : Lessons Learned from Practice. , 2016.</p>
				<p>GUERREIRO, T. Accessibility layers. <strong>ACM SIGACCESS Accessibility and Computing</strong>, n. 113, p. 22–28, 2015.</p>
				<p>HAKOBYAN, L.; LUMSDEN, J.; BARTLETT, H. Mobile assistive technologies for the visually impaired. <strong>Survey of Ophthalmology</strong>, v. 58, p. 513–528, 2013. Disponível em: <a href="http://dx.doi.org/10.1016/j.survophthal.2012.10.004" target="blank">http://dx.doi.org/10.1016/j.survophthal.2012.10.004</a>. Acesso em: 2/10/2018.</p>
				<p>HUDEC, M.; SMUTNY, Z. RUDO: A home ambient intelligence system for blind people. <strong>Sensors (Switzerland)</strong>, v. 17, n. 8, 2017.</p>
				<p>JAIMES, A.; SEBE, N. Multimodal human–computer interaction: A survey. <strong>Computer Vision and Image Understanding</strong>, v. 108, n. 1–2, p. 116–134, 2007. Disponível em: <a href="www.elsevier.com/locate/cviu" target="blank">www.elsevier.com/locate/cviu</a>. Acesso em: 26/2/2019.</p>
				<p>KUMAR, S.; SANAMAN, G. Web challenges faced by blind and vision impaired users in libraries of Delhi. <strong>The Electronic Library</strong>, v. 33, n. 2, p. 242–257, 2015. Disponível em: <a href="https://doi.org/10.1108/EL-03-2013-0043" target="blank">https://doi.org/10.1108/EL-03-2013-0043</a>. Acesso em: 27/9/2018.</p>
				<p>LAHAV, O.; HAGAB, N.; EL KADER, S. A.; LEVY, S. T.; TALIS, V. Listen to the models: Sonified learning models for people who are blind. <strong>Computers & Education</strong>, v. 127, p. 141–153, 2018. Disponível em: <a href="https://linkinghub.elsevier.com/retrieve/pii/S0360131518302240" target="blank">https://linkinghub.elsevier.com/retrieve/pii/S0360131518302240</a>. Acesso em: 15/6/2019.</p>
				<p>LIU, Y.; STILES, N. R.; MEISTER, M. Augmented reality powers a cognitive assistant for the blind. <strong>eLife</strong>, v. 7, 2018. Disponível em: <a href="https://elifesciences.org/articles/37841" target="blank">https://elifesciences.org/articles/37841</a>. Acesso em: 14/6/2019.</p>
				<p>LOIACONO, E. T.; DJAMASBI, S.; KIRYAZOV, T. Factors that affect visually impaired users’ acceptance of audio and music websites. <strong>Journal of Human Computer Studies</strong>, v. 71, p. 321–334, 2012. Disponível em: <a href="www.sciencedirect.com" target="blank">www.sciencedirect.com</a>.</p>
				<p>MAĆKOWSKI, M. S.; BRZOZA, P. F.; SPINCZYK, D. R. Tutoring math platform accessible for visually impaired people. <strong>Computers in Biology and Medicine</strong>, v. 95, p. 298–306, 2018. Disponível em: <a href="http://dx.doi.org/10.1016/j.compbiomed.2017.06.003" target="blank">http://dx.doi.org/10.1016/j.compbiomed.2017.06.003</a>. Acesso em: 3/10/2018.</p>
				<p>MELIONES, A.; SAMPSON, D. Blind MuseumTourer: A System for Self-Guided Tours in Museums and Blind Indoor Navigation. <strong>Technologies</strong>, v. 6, n. 1, p. 4, 2018.</p>
				<p>MUKHERJEE; GARAIN, U.; BISWAS, A. Experimenting with Automatic Text-to-Diagram Conversion: A Novel Teaching Aid for the Blind People. <strong>Educational Technology & Society</strong>, v. 17, n. 3, p. 1176–3647, 2014. Disponível em: <a href="https://pdfs.semanticscholar.org/a5cc/c65fa79b6e8a4f16c667ee1c76ff0c27a4e3.pdf?_ga=2.171031653.479329528.1560638669-2146878616.1548338717" target="blank">https://pdfs.semanticscholar.org/a5cc/c65fa79b6e8a4f16c667ee1c76ff0c27a4e3.pdf?_ga=2.171031653.479329528.1560638669-2146878616.1548338717</a>. Acesso em: 15/6/2019.</p>
				<p>NAHAR, L.; JAAFAR, A.; AHAMED, E.; KAISH, A. B. M. A. Design of a Braille Learning Application for Visually Impaired Students in Bangladesh. <strong>Assistive Technology</strong>, v. 27, n. 3, p. 172–182, 2015. Disponível em: <a href="http://www.tandfonline.com/doi/full/10.1080/10400435.2015.1011758" target="blank">http://www.tandfonline.com/doi/full/10.1080/10400435.2015.1011758</a>.</p>
				<p>PARK, C. W.; ALDERMAN, J. <strong>Designing across senses : a multimodal approach to product design.</strong> 1o ed. Sebastopol: O’Rilley Media, Inc, 2018.</p>
				<p>PASSERINO, L. M.; MONTARDO, S. P. Inclusão Digital e Acessibilidade Digital: Interfaces e aproximações conceituais. XVI Encontro da Compós - Associação Nacional dos Programas de Pós-Graduação em Comunicação. <strong>Anais...</strong>, 2007. Curitiba. Disponível em: <a href="http://redessociaiseinclusao.pbworks.com/f/ID-acess_compos_2007_versão final.pdf" target="blank">http://redessociaiseinclusao.pbworks.com/f/ID-acess_compos_2007_versão final.pdf</a>. Acesso em: 17/6/2019.</p>
				<p>POWER, C.; JÜRGENSEN, H.; JU, H.; JURGENSEN, H. Accessible presentation of information for people with visual disabilities. <strong>Universal Access in the Information Society</strong>, v. 9, n. 2, p. 97–119, 2009. Disponível em: <a href="http://link.springer.com/content/pdf/10.1007/s10209-009-0164-1.pdf" target="blank">http://link.springer.com/content/pdf/10.1007/s10209-009-0164-1.pdf</a>. Acesso em: 3/4/2013.</p>
				<p>TAKAGI, H.; SAITO, S.; FUKUDA, K.; ASAKAWA, C. Analysis of navigability of Web applications for improving blind usability. <strong>ACM Transactions on Computer-Human Interaction</strong>, v. 14, n. 3, p. 13- es, 2007. ACM. Disponível em: <a href="http://portal.acm.org/citation.cfm?doid=1279700.1279703" target="blank">http://portal.acm.org/citation.cfm?doid=1279700.1279703</a>. Acesso em: 17/6/2019.</p>
				<p>W3C. Web Content Accessibility Guidelines (WCAG) 2.0. Disponível em: <a href="https://www.w3.org/TR/WCAG20/" target="blank">https://www.w3.org/TR/WCAG20/</a>. Acesso em: 31/1/2018.</p>







				</article>
			</section>
		</main>
		<footer>
			<section class="container">
					<a class="esquerda" href="index.html">início</a>
					<!-- <a class="esquerda" href="fichatecnica.html">ficha técnica</a> -->
					
					<!-- avançar -->
					<a class="direita" href="artigo_3.html">&#9654</a>
					
					<!-- voltar -->
					<a class="direita" href="artigo_5.html">&#9664</a>
					
			</section>
		</footer>
	
</body>
	


</html>




